{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as psf\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import VectorAssembler\nimport pyspark.ml.evaluation as evals\nimport pyspark.ml.tuning as tune\nfrom pyspark.ml import Pipeline\nimport pandas as pd\nimport numpy as np\ntest = spark.read.csv('FileStore/tables/test.csv', header = True)\ntrain = spark.read.csv('FileStore/tables/train.csv', header = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#1) data cleaning\n#function for finding number of NAs in each column, to see if interpolation, data dropping, needed | add: min, max, mean, presence of outliers etc. to see if if/where cleaning necessary\ndef clean_info(df):\n  n_col = len(df.columns)\n  n_row = df.count()\n  #initializing results dataframe\n  data = {'Name':df.columns, 'Nas':np.repeat(0, n_col), 'Percent':np.repeat(0,n_col)} \n  result = pd.DataFrame(data) \n  #loop through each column and get number of NAs\n  for i in range(0, n_col):\n    #name\n    result.iloc[i,0] = df.columns[i]\n    #NAs\n    result.iloc[i,1] = n_row - df.select(df.columns[i]).drop().count()\n    #perc\n    result.iloc[i,2] = result.iloc[i,1] / n_row\n  return result\n\n#this data is already cleaned, so no NAs, erroneous data, outliers, etc.\nclean_info(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>\n                Name  Nas  Percent\n0                key    0      0.0\n1        fare_amount    0      0.0\n2    pickup_datetime    0      0.0\n3   pickup_longitude    0      0.0\n4    pickup_latitude    0      0.0\n5  dropoff_longitude    0      0.0\n6   dropoff_latitude    0      0.0\n7    passenger_count    0      0.0\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["#2) feature engineering\n#function to create features, in order to be able to apply to test set later on\ndef create_features(df):\n  #converting strings to doubles\n  columns = ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n  for i in columns:\n    df = df.withColumn(i, df[i].cast('double'))\n  \n  #change pickup datetime to datetime from string, day of week\n  df.createOrReplaceTempView('data')\n  to_datetime = \"SELECT *, TO_TIMESTAMP(SUBSTRING(pickup_datetime, 1, 19)) AS pickup_datetime_new, DATE_FORMAT(TO_TIMESTAMP(SUBSTRING(pickup_datetime, 1, 19)), 'EEEE') AS dow FROM data\"\n  df = spark.sql(to_datetime)\n  \n  #create columns for year, month, day, hour, minute\n  df = df.withColumn('year', psf.year(df.pickup_datetime_new))\n  df = df.withColumn('month', psf.month(df.pickup_datetime_new))\n  df = df.withColumn('day', psf.dayofmonth(df.pickup_datetime_new))\n  df = df.withColumn('hour', psf.hour(df.pickup_datetime_new))\n  df = df.withColumn('minute', psf.minute(df.pickup_datetime_new))\n  \n  #converting day of week to one hot encoding\n  df = df.withColumn('dow_array', psf.split(psf.col('dow'),' '))\n  dowVectorizer = CountVectorizer(inputCol='dow_array', outputCol='dow_one_hot', vocabSize=7, minDF=1.0)\n  dowVectorizer_model = dowVectorizer.fit(df)\n  df = dowVectorizer_model.transform(df)\n  \n  #holidays, static ones, could encode year specific or use library\n  holidays = [(1,1),(7,4),(12,25)]\n  df = df.withColumn('holiday', psf.lit(0))\n  for i in range(0, len(holidays)):\n    df = df.withColumn('holiday', ((df.day == holidays[i][1]) & (df.month == holidays[i][0])) | (df.holiday == True))\n  \n  #night time surcharge beween 20 and 6\n  df = df.withColumn('night', (df.hour >= 20) | (df.hour <= 6))\n  \n  #dropping unnecessary columns\n  drops = ['key', 'pickup_datetime', 'pickup_datetime_new', 'dow', 'dow_array']\n  for i in drops:\n    df = df.drop(i)\n    \n  #drop nas\n  df = df.dropna()\n  \n  #drop fares less than equal to 0\n  df = df.filter(df.fare_amount > 0)\n  \n  #creating label column, equal to fare_amount\n  df = df.withColumn('label', df.fare_amount)\n  \n  return df\n\n#creating the engineered training data\ntraining_data = create_features(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#3) fitting models\n#function for easily testing new algorithms\ndef test_algorithm(data, model, grid, evaluator_string, folds):\n  #data: df with data, model: a pyspark.ml model object, grid: a tuning grid, evaluator_string: a string for the evaluator type to be used, folds: integer for k number of folds for cross validation\n  \n  #vector assembler\n  vec_assembler = VectorAssembler(inputCols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 'hour', 'minute', 'dow_one_hot', 'holiday', 'night'], outputCol = 'features')\n  \n  #pipeline\n  taxi_pipe = Pipeline(stages=[vec_assembler])\n  piped_data = taxi_pipe.fit(data).transform(data)\n  training, test = piped_data.select(['features', 'label']).randomSplit([.8, .2])\n  \n  #evaluator\n  evaluator = evals.RegressionEvaluator(metricName=evaluator_string)\n  \n  #cross validator\n  cv = tune.CrossValidator(estimator=model,\n               estimatorParamMaps=grid,\n               evaluator=evaluator,\n               numFolds=folds\n               )\n  #fit the cross-validated model\n  models = cv.fit(training)\n  best_model = models.bestModel\n  \n  #evaluate on the test set\n  predictions = best_model.transform(test)\n  evaluation = evaluator.evaluate(predictions)\n  \n  #best parameters\n  best_params = best_model.extractParamMap()\n  \n  #returning model and metrics\n  return best_model, evaluation, best_params"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#decision tree\nfrom pyspark.ml.regression import DecisionTreeRegressor\ndt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'label')\n\n#grid search\ndt_grid = tune.ParamGridBuilder()\ndt_grid = dt_grid.addGrid(dt.maxDepth, np.arange(5, 6))\ndt_grid = dt_grid.build()\n\n#evaluate the algorithm\ndt_model, dt_eval, dt_best_params = test_algorithm(training_data, dt, grid, 'mse')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["#linear regression\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(featuresCol = 'features', labelCol='label')\n\n#grid search\nlr_grid = tune.ParamGridBuilder()\nlr_grid = lr_grid.addGrid(lr.maxIter, np.arange(5, 11, 5))\nlr_grid = lr_grid.addGrid(lr.elasticNetParam, [0,1])\nlr_grid = lr_grid.build()\n\n#evaluate the algorithm\nlr_model, lr_eval, dt_best_params = test_algorithm(training_data, lr, lr_grid, 'mse')"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"new-york-city-taxi-fare-prediction","notebookId":3349979285127688},"nbformat":4,"nbformat_minor":0}
