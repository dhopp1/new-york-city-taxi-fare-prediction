{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as psf\nfrom pyspark.ml.feature import CountVectorizer\nimport pandas as pd\nimport numpy as np\ntest = spark.read.csv('FileStore/tables/test.csv', header = True)\ntrain = spark.read.csv('FileStore/tables/train.csv', header = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#1) data cleaning\n#function for finding number of NAs in each column, to see if interpolation, data dropping, needed | add: min, max, mean, presence of outliers etc. to see if if/where cleaning necessary\ndef clean_info(df):\n  n_col = len(df.columns)\n  n_row = df.count()\n  #initializing results dataframe\n  data = {'Name':df.columns, 'Nas':np.repeat(0, n_col), 'Percent':np.repeat(0,n_col)} \n  result = pd.DataFrame(data) \n  #loop through each column and get number of NAs\n  for i in range(0, n_col):\n    #name\n    result.iloc[i,0] = df.columns[i]\n    #NAs\n    result.iloc[i,1] = n_row - df.select(df.columns[i]).drop().count()\n    #perc\n    result.iloc[i,2] = result.iloc[i,1] / n_row\n  return result\n\n#this data is already cleaned, so no NAs, erroneous data, outliers, etc.\nclean_info(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#2) feature engineering\n#function to create features, in order to be able to apply to test set later on\ndef create_features(df):\n  #change pickup datetime to datetime from string, day of week\n  df.createOrReplaceTempView('data')\n  to_datetime = \"SELECT *, TO_TIMESTAMP(SUBSTRING(pickup_datetime, 1, 19)) AS pickup_datetime_new, DATE_FORMAT(TO_TIMESTAMP(SUBSTRING(pickup_datetime, 1, 19)), 'EEEE') AS dow FROM data\"\n  df = spark.sql(to_datetime)\n  \n  #create columns for year, month, day, hour, minute\n  df = df.withColumn('year', psf.year(df.pickup_datetime_new))\n  df = df.withColumn('month', psf.month(df.pickup_datetime_new))\n  df = df.withColumn('day', psf.dayofmonth(df.pickup_datetime_new))\n  df = df.withColumn('hour', psf.hour(df.pickup_datetime_new))\n  df = df.withColumn('minute', psf.minute(df.pickup_datetime_new))\n  \n  #converting day of week to one hot encoding\n  df = df.withColumn('dow_array', psf.split(psf.col('dow'),' '))\n  dowVectorizer = CountVectorizer(inputCol='dow_array', outputCol='dow_one_hot', vocabSize=7, minDF=1.0)\n  dowVectorizer_model = dowVectorizer.fit(df)\n  df = dowVectorizer_model.transform(df)\n  \n  #holidays, static ones, could encode year specific or use library\n  holidays = [(1,1),(7,4),(12,25)]\n  df = df.withColumn('holiday', psf.lit(0))\n  for i in range(0, len(holidays)):\n    df = df.withColumn('holiday', ((df.day == holidays[i][1]) & (df.month == holidays[i][0])) | (df.holiday == True))\n  \n  #night time surcharge beween 20 and 6\n  df = df.withColumn('night', (df.hour >= 20) | (df.hour <= 6))\n  \n  #dropping unnecessary columns\n  drops = ['key', 'pickup_datetime', 'pickup_datetime_new', 'dow', 'dow_array']\n  for i in drops:\n    df = df.drop(i)\n  \n  return df\n\n#creating the engineered training data\ntraining_data = create_features(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#3) create pipeline\n#4) train test split, create model\n#5) cross validation and grid search parameters\n#6) select the best model\n#7) predict"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------------+---------------+-----------------+-----------------+---------------+----+-----+---+----+------+-------------+-------+-----+\nfare_amount|pickup_longitude|pickup_latitude|dropoff_longitude| dropoff_latitude|passenger_count|year|month|day|hour|minute|  dow_one_hot|holiday|night|\n+-----------+----------------+---------------+-----------------+-----------------+---------------+----+-----+---+----+------+-------------+-------+-----+\n        4.5|      -73.844311|      40.721319|        -73.84161|40.71227800000001|              1|2009|    6| 15|  17|    26|(7,[6],[1.0])|  false|false|\n       16.9|      -74.016048|      40.711303|       -73.979268|        40.782004|              1|2010|    1|  5|  16|    52|(7,[4],[1.0])|  false|false|\n        5.7|      -73.982738|       40.76127|       -73.991242|        40.750562|              2|2011|    8| 18|   0|    35|(7,[2],[1.0])|  false| true|\n+-----------+----------------+---------------+-----------------+-----------------+---------------+----+-----+---+----+------+-------------+-------+-----+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":4}],"metadata":{"name":"new-york-city-taxi-fare-prediction","notebookId":976483796334320},"nbformat":4,"nbformat_minor":0}
